{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilhamydn17/2141720091-MachLearn-2023/blob/week-10/Praktikum_2_%26_TUGAS_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmdd6QCpkQpc"
      },
      "source": [
        "## Praktikum 2 - RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MTpdEIUnkFYA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "PlL2v56NaLZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28s6TBQaMfw",
        "outputId": "9d0b6bb8-9762-4ebf-a796-eaaa5a8f94f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUQQ6SeFaVUv",
        "outputId": "bdebac61-2869-400c-a22e-8ba6531a46b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDKzf5EiaYoa",
        "outputId": "6f450b6e-9310-4ea3-960d-fca7e6027238"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3F8r_AVacnK",
        "outputId": "9c569895-2e54-4d37-80b9-824b429f3741"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "SV-78mGgagTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARSMsaMjajQi",
        "outputId": "71923c83-d8a2-44ae-f469-35c2fd341e63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "oiokgtVNaonz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymxh3DRzasTX",
        "outputId": "82f40885-36f3-46f0-d7bd-877c501dd70e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "gVarNk_daxPs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJFhnFGmay_u",
        "outputId": "693eaef1-ac30-48f9-cf01-f0d5af622a5e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9evYRCrPbN3y",
        "outputId": "4679e697-28a8-430b-de1c-f83aa5e30e97"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "nE9pt62dbnFU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ZK1JIXbuO1",
        "outputId": "fe5814c2-d99b-4939-efcd-70e959aee06f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "BYP3nw28bxzQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbDkjaHjb23V",
        "outputId": "fd6f9665-8d95-455a-863c-d3f2b7ce948b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "Y2zwv5Gvb6IU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qQbCz-Gb_su",
        "outputId": "67abc7c6-aeeb-4e32-cd78-e2b56d23c2e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjmsBPOVcKzI",
        "outputId": "fb7e0fbc-f95c-449a-d578-8b8e0db773f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "XJEXJBTLcPZy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1ongAsXcTKh",
        "outputId": "2e65f312-5c26-4b74-8699-e9910d1c1940"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "qkUWdgSDcVBs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzkrkw3LcX3b",
        "outputId": "be614af3-e6db-4a04-c244-4793f5f55f95"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1KnKrjRcbOw",
        "outputId": "dbc7f3ae-d22a-45f0-87bd-66d7ba856262"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "MnsS1rE5cekv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "dihWKdibciXj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "j6O5645cck1_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxQFjaxEco91",
        "outputId": "7edafdc9-1713-4134-b3e5-40346b505139"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnd8VyD6csJz",
        "outputId": "4fafe670-833d-4c2c-80ab-9f9d873891d1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "StC0j8jNcte1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxezvEuWcw0L",
        "outputId": "0e556f5c-6ee0-49ae-b139-ce37cd0d551d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b's yet I do not: but, as I can learn,\\nHe hearkens after prophecies and dreams;\\nAnd from the cross-row'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'QvX[UNK]utoVV3k-wV,\\n$utTQ.BZ:aGZcoTJLil[UNK]qqG-putxEUgCcDNWlPOlQCqUw:PetU,ilCjn-qAFI?ERm?CsL$rMFEGVVs R\\nYhy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "GfPJsQx9c5G2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5qVrT7Mc6co",
        "outputId": "4e26c2a4-7324-4c99-c56c-610c71f3e5a2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1905766, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKW6l-Jpd1zq",
        "outputId": "efa525cf-caec-4484-d7d2-ff7129c22dff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.06087"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "oT57fA9Hd3b4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "avnUYUqBd4kk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "W5O3PNuHd8MI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITA4W9Bod-hB",
        "outputId": "15db79e2-95f2-4563-92f5-10bf7ed2a709"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 19s 55ms/step - loss: 2.7053\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.9792\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.6998\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.5420\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.4453\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.3789\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3271\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2823\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2418\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2011\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1614\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1199\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.0757\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.0295\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9805\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9292\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8769\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8240\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7720\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "Wijtl-SzeAuu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "7iRZFzgdfTvV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtrVjym5fVeH",
        "outputId": "cb6519e2-d5a9-418a-e28a-6a4e86e87e28"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Come to me: you know the manacles of our bitter\n",
            "continuent sup ill-body, and Some puts and\n",
            "To call me some scather, brainly stay.\n",
            "\n",
            "Nurse:\n",
            "Ay, brain! why? Now, afore God for best\n",
            "To serve thy conscience to me from fasting yet in one to age\n",
            "thy counsel's knees. Behold the royal dry too late\n",
            "I give unto be moght, and, whilst I play't\n",
            "And take the souls rid on san so nink.\n",
            "\n",
            "YORK:\n",
            "They should knew by his remedy?\n",
            "\n",
            "BISHOP OF CARLISLE:\n",
            "My lord.\n",
            "\n",
            "CORIOLANUS:\n",
            "Let them have watch'd, with no less cancelliness\n",
            "That kiss the sweets? adden his lessons\n",
            "And manner to do so remove\n",
            "As the shepherds do with a lover.\n",
            "\n",
            "TRANIO:\n",
            "So this is Ludy steel.\n",
            "\n",
            "LEONTES:\n",
            "Hark!\n",
            "\n",
            "First Senator:\n",
            "The conscience sir,\n",
            "Your tributary deeds must wish one for tyick,\n",
            "And then draw interrupts, these weeps,\n",
            "Which makes me down, all in your hateful prince.\n",
            "\n",
            "WARWICK:\n",
            "And till the duke my husband--whither York,\n",
            "No better woadd! he wakes us well.\n",
            "I will be hurt'd with nothing.\n",
            "More legst you speak upon't!\n",
            "What, do you jealous under h \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.6188979148864746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAORDe4xfW8O",
        "outputId": "2c21528d-3e13-4582-96ea-134653673925"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nGrimmNord's in a man'st friend.\\nLet them plant but when I seem cowards her love?\\nWhere's Clifford duty undischarged at unpregame?\\n\\nTailor:\\nWhy I should bid him or no happy dry he of such\\nBirst against thy brother veil'd than steal\\nThan party from the light tomb company.\\n\\nABPHOSER:\\nLet me enjoy' with mildness arms.\\n\\nBRUTUS:\\nLet them but tell me, I'll be your king now. Where brother,\\nA wife to look here, and this is England's king\\nHath now his son for and name of get:\\nUnto my vice is nothing; for the other flours\\nAnd brought it, even here I came from year,\\nSo shalt he be made for moved to begg\\na beggars.\\n\\nPROSPERO:\\nTwife is banish'd; and he shall be cross'd.\\n\\nWARWICK:\\nSoft father Edward Bianca, stand alood,\\nYou shall harm in her present, course of justice;\\nWhere, ere we now? what with a fault? Who's there?\\n\\nKING RICHARD II:\\nRight.\\n\\nARCHARDINEN:\\nA dig!\\nTurn gild, my wife, and leave his native years,\\nUnto his revolit, we should have heard to-day;\\nFor partures he have scarce vesses, or for\"\n",
            " b\"ROMEO:\\nMy lord, do not so happy better two not fine;\\nLike pine thine for Claudio is there,\\nWhen it is clogly and against my heart\\nAnGegiles nor heaven sity,\\nthen musilian stops his pastion, so rich in solemn,\\nSuch as thou say'st me not, so it is dead.\\n\\nSEBASTIAN:\\nYou would the world through his honour, to scell,\\nI cannot judge;\\nAnd not with mains hands this deep suil;\\nIt were disfedacies and dog, that revenge\\nWith rude men have a happy drivellows throne.\\n\\nGLOUCESTER:\\nMakes he that knew she stood in scorn Saint George!'\\n\\nMERCUTIO:\\nWhere lies it, like to me?--but think,--\\n\\nKING LEWIS XI:\\nNow proud that are they but for her breath,\\nWhich, being natures in the wars. Come, made he make\\nBreak off that which if he was the stanks,\\nSpirits no want pairs draws.\\n\\nLORD WELBY:\\nBase it that shall might there be here and the place what might\\nBe not adain. Dear meaning, not unlike,\\nI spy all too much simples of his crown confound\\nWhat same policy of such a fellow,\\nNot jump them in the bridal delly days;\\nAnd\"\n",
            " b\"ROMEO:\\nTrade heir tongues into this France is dangerous;\\nSome slave fear they shall find that thou shalt have\\nDoubly rascal crowns.\\n\\nLEONTES:\\nHa! what company?\\n\\nAfterward:\\nAnd we knew not what do this my soul.\\n\\nGood Lucines: these hath did usurp\\nA world of every good deeds open.\\nIf this I care not what this name as of him?\\n\\nLADY GREY:\\n'Twill give you a numble at the Tower?\\n\\nBENVOLIO:\\nWhy, here's a coddid talk.\\n\\nCALIBAN:\\nNo, my reason, whom I did gird; with papity hasting image\\nWater and forks drums. You justle, forbear!\\nIf she doth over effect this happiness.\\n\\nBUCKINGHAM:\\nHe did, my liege, in hear of Justive?\\n\\nDUKE OF YORK:\\nPeace is back to bring yet put in her,\\nDiding remote, unless ye being\\nnow attending queen. The king is and one that well assure\\nThe nurse of kings, and not a string her own.\\n\\nPage:\\nMy affections are touch'd; in mine honour, hath begun,\\nHis labour is mine opinion, come again.\\n\\nROMEO:\\nFather, ladies, like a day, and a foolish'd most\\nis made a vassal, hopeth words o' the bas\"\n",
            " b\"ROMEO:\\nAlack the horse upon her, nurse.\\n\\nDUCHESS OF YORK:\\nNo, not to sin, save your fellow'st true.\\n\\nPOMPEY:\\nMadam. But her fellow that's such.\\nShame the time! Prithee end, now all my head\\nAnd then appointed scorns disgrace, duke,\\nBut sunds to rash the villain's earth,\\nAnd do not taken words from are the vext\\nThat dict to Corioli be a hundred deg.\\nUnbliant murning, in the vapour of my heart\\nThus pass'd your courther, I.\\n\\nQUEEN MARGARET:\\nBring this way, made the cruftsmaid sought\\nTo have a roper with maintance and\\nSubject no interription to tell,\\nUnless commits me to the death.\\nWede he money, on my trembling woe?\\nUnsheathe him and make known her life-bear.\\nWhich to determiny of this court?\\n\\nThird Citizen:\\nYou have a happy day, it do pursue;\\nAnd I am dirncion, since fire your patience;\\nIf he could bear your brothers, and their brows, news, my trie;\\nFor word 'marries an action of his head.\\n\\nDUKE OF YORK:\\nMake haste, make heaven me in my tongue even in love\\nThat I myself in his neck; and then\\nI \"\n",
            " b\"ROMEO:\\nAmen, dear, princess King of Naples.\\n\\nDUKE VINCENTIO:\\nNay, let them needs we needed hel;\\nBut O, this hour by my and mistrust,\\nBut only he has observed in him\\nThat all the northern Early thirsty has subed\\nTo pardon him, and he shall make gase as weak.\\nNow, when they seemed as this Edward.\\n\\nKING EDWARD IV:\\nHow say it is mute words? Well as they say?\\nThis cannot, for this once more torce. Thou minstrous,\\nLike a followned mellow hast neited he\\ntrees merriment; it that art thou bring me hence,\\nAnd yet they blad it private in such particles\\nDid thou his stands by day.\\n\\nLUCIO:\\nAnd I from time to join with those myself makes\\nTo trust their garments are nicedal.\\n\\nCLARENCE:\\nCome hither, Bravincaling a lander's liberal,\\nAnd he shall spend mine other man.\\n\\nRICHMOND:\\nGood night, as in a dame; but It would know\\nHow do I proved this young liking\\nAgainst my struck did fast for wholity.\\n\\nKING RICHARD III:\\nMy noble cousin, stay the constant may and fear\\nThat any which yet she seeks committed, and am th\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.285738945007324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH_XMudcfaLq",
        "outputId": "a72169a5-9cd8-411b-8d44-eadd99f23b6b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e2594da5fc0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujlLv3rIfb_5",
        "outputId": "f7da3b47-02b3-4acb-d158-f724e2ca0925"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "She may, my lord, think you:\n",
            "if you shall come to the people, not with law;\n",
            "Remom his ghost in pala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas Praktikum"
      ],
      "metadata": {
        "id": "eoSCfq_S_rah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "id": "eObNIcTyfeDb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "  vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "oJy8ZZ7yfpkI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "rZl5aiQ6fsNb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhZN25lqfvYm",
        "outputId": "2331e74d-90d6-49b1-a7bb-a5bcbfa27829"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 58ms/step - loss: 2.7021\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e256cc7a2f0>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Owo127kfxBK",
        "outputId": "bf784cc6-94c9-4fe1-a28a-3d616a7e6c23"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1720\n",
            "Epoch 1 Batch 50 Loss 2.0585\n",
            "Epoch 1 Batch 100 Loss 1.9523\n",
            "Epoch 1 Batch 150 Loss 1.8397\n",
            "\n",
            "Epoch 1 Loss: 1.9776\n",
            "Time taken for 1 epoch 12.88 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7987\n",
            "Epoch 2 Batch 50 Loss 1.7510\n",
            "Epoch 2 Batch 100 Loss 1.6993\n",
            "Epoch 2 Batch 150 Loss 1.6618\n",
            "\n",
            "Epoch 2 Loss: 1.6999\n",
            "Time taken for 1 epoch 11.79 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5536\n",
            "Epoch 3 Batch 50 Loss 1.5854\n",
            "Epoch 3 Batch 100 Loss 1.5495\n",
            "Epoch 3 Batch 150 Loss 1.5125\n",
            "\n",
            "Epoch 3 Loss: 1.5420\n",
            "Time taken for 1 epoch 11.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4976\n",
            "Epoch 4 Batch 50 Loss 1.4258\n",
            "Epoch 4 Batch 100 Loss 1.4511\n",
            "Epoch 4 Batch 150 Loss 1.3881\n",
            "\n",
            "Epoch 4 Loss: 1.4463\n",
            "Time taken for 1 epoch 11.61 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3734\n",
            "Epoch 5 Batch 50 Loss 1.4061\n",
            "Epoch 5 Batch 100 Loss 1.3564\n",
            "Epoch 5 Batch 150 Loss 1.3471\n",
            "\n",
            "Epoch 5 Loss: 1.3791\n",
            "Time taken for 1 epoch 11.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3476\n",
            "Epoch 6 Batch 50 Loss 1.3493\n",
            "Epoch 6 Batch 100 Loss 1.3481\n",
            "Epoch 6 Batch 150 Loss 1.3198\n",
            "\n",
            "Epoch 6 Loss: 1.3271\n",
            "Time taken for 1 epoch 11.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2698\n",
            "Epoch 7 Batch 50 Loss 1.2962\n",
            "Epoch 7 Batch 100 Loss 1.2689\n",
            "Epoch 7 Batch 150 Loss 1.3172\n",
            "\n",
            "Epoch 7 Loss: 1.2828\n",
            "Time taken for 1 epoch 11.31 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2937\n",
            "Epoch 8 Batch 50 Loss 1.2141\n",
            "Epoch 8 Batch 100 Loss 1.2308\n",
            "Epoch 8 Batch 150 Loss 1.2981\n",
            "\n",
            "Epoch 8 Loss: 1.2419\n",
            "Time taken for 1 epoch 11.95 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1531\n",
            "Epoch 9 Batch 50 Loss 1.2315\n",
            "Epoch 9 Batch 100 Loss 1.1901\n",
            "Epoch 9 Batch 150 Loss 1.2202\n",
            "\n",
            "Epoch 9 Loss: 1.2019\n",
            "Time taken for 1 epoch 12.40 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1575\n",
            "Epoch 10 Batch 50 Loss 1.1519\n",
            "Epoch 10 Batch 100 Loss 1.1704\n",
            "Epoch 10 Batch 150 Loss 1.1679\n",
            "\n",
            "Epoch 10 Loss: 1.1610\n",
            "Time taken for 1 epoch 12.46 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soal\n",
        "Berdasarkan kode dan 2 kelas yang telah dilakukan diatas, dapat dilihat perbedaanya seperti berikut:\n",
        "\n",
        "1. Tujuan Kelas\n",
        "  * ``OneStep``: Tujuan dari kelas OneStep adalah untuk melakukan satu langkah generasi teks, mengambil teks input sebelumnya, dan menghasilkan satu karakter atau token berikutnya dalam teks yang akan digenerate. Kelas ini digunakan untuk menghasilkan teks berdasarkan model bahasa yang telah dipelajari.\n",
        "  * ``CustomTraining``: Tujuan dari kelas CustomTraining adalah untuk melatih model bahasa. Kelas ini digunakan untuk melakukan langkah pelatihan yang mencakup perhitungan loss, perhitungan gradien, dan pembaruan parameter-model.\n",
        "2. Metode Utama\n",
        "  * ``OneStep``: memiliki metode utama ``generate_one_step``, yang akan mengambil teks input sebelumnya dan menghasilkan karakter dan token berikutnya dalam teks berdasarkan model bahasa.\n",
        "  * ``CustomTraining``: Metode utama dalam kelas CustomTraining adalah train_step. Metode ini digunakan selama pelatihan model dan mencakup perhitungan loss, perhitungan gradien, dan pembaruan parameter-model.\n",
        "3. Prosedur Pelatihan\n",
        "  * ``OneStep``: kelas ini tidak terlibat dalam proses pelatihan model, hanya digunakan untuk menghasilkan teks berdasarkan model yang telah dipelajari.\n",
        "  * ``CustomTraining``: kelas ini digunanakan selama proses pelatihan model, yang melakukan perhitungan loss, gradien, dan pembaruan paramater.\n",
        "4. Output Hasil Pelatihan\n",
        "  * ``OneStep``: menghasilkan karakter atau token berikutnya dalam teks yang akan digenerate. Output tersebut merupakan teks yang dihasilkan model berdasarkan teks input sebelumnya.\n",
        "  * ``Custom Training``: menghasilkan output dengan loss yang dihasilkan saat proses pelatihan, yang mana nilai tersebut digunakan untuk mengukur sejauh mana prediksi model sesuai dengan label yang seharusnya.\n",
        "5. Durasi Waktu Eksekusi Kode\n",
        "  * Durasi waktu eksekusi kodenya akan berbeda antara kedua kelas, hal tersebut karena bergantung pada kompleksitas perhitungan yang dilakukan. Kelas ``OneStep`` hanya melakukan satu langkah generasi, yang dapat dilakukan dengan cepat, sedangkan kelas ``CustomTraining`` melakukan lebih banyak perhitungan selama proses pelatihan, sehingga waktu eksekusinya mungkin lebih lama.\n",
        "6. Akurasi Prediksi\n",
        "  * ``OneStep`` : Akurasi prediksi dalam kelas OneStep sulit untuk dievaluasi secara langsung karena tidak terlibat dalam proses pelatihan model. Fokusnya kelas ini hanya menghasilkan teks yang koheren dan berkualitas tinggi, bukan memprediksi bagaimana hasil dari output.\n",
        "  * ``CustomTraining``: Selama proses pelatihan, model menghasilkan prediksi teks, dan akurasi prediksi dinilai berdasarkan sejauh mana prediksi model sesuai dengan label yang seharusnya. Output dari kelas CustomTraining adalah loss yang dihasilkan selama proses pelatihan, yang mana nilai tersebut digunakan untuk mengukur sejauh mana prediksi model sesuai dengan label. Semakin rendah nilai loss, semakin baik akurasi prediksi model.\n",
        "\n",
        "  ****\n",
        "\n"
      ],
      "metadata": {
        "id": "BbobJrBqiLLI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}